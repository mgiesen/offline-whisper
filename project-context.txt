<project_overview>
<generated_at>Offline-Whisper</generated_at>

<directory_structure>
Offline-Whisper/
│  .github/
│     workflows/
│     └─ docker-publish.yml
│  deploy/
│  ├─ preload-models.py
│  nginx/
│  │  public/
│  │  └─ index.html
│  ├─ Dockerfile
│  readme/
│  whisper-api/
│  ├─ Dockerfile
│  ├─ main.py
│  └─ requirements.txt
├─ docker-compose.yml
├─ Dockerfile
├─ LICENSE
└─ README.md
</directory_structure>

<file_contents>

<file path="docker-compose.yml">
services:
  whisper:
    build: ./whisper-api
    restart: unless-stopped

  webserver:
    build: ./nginx
    ports:
      - "8077:8077"
    depends_on:
      - whisper
    restart: unless-stopped
</file>

<file path="Dockerfile">
# Basis-Image
FROM python:3.9-slim

# Systempakete installieren: ffmpeg, nginx, supervisor
RUN apt-get update && apt-get install -y --no-install-recommends \
    ffmpeg \
    nginx \
    supervisor

# Arbeitsverzeichnis setzen
WORKDIR /app

# Kopiere Dateiordner der Services in das Image
COPY whisper-api/ ./whisper-api/
COPY nginx/ ./nginx/
COPY deploy/preload-models.py ./deploy/
COPY deploy/supervisord.conf /etc/supervisor/conf.d/supervisord.conf

# Python-Abhängigkeiten für die Whisper-API installieren
RUN pip install --no-cache-dir -r whisper-api/requirements.txt

# Whisper-Modelle vorladen
RUN python deploy/preload-models.py

# Nginx-Konfiguration und Dateien kopieren (überschreibt die Default-Konfiguration)
COPY nginx/single-container.conf /etc/nginx/conf.d/default.conf
COPY nginx/public/ /usr/share/nginx/html/

# Unnötige Dateien entfernen, um das Image zu verschlanken
RUN rm -rf /app/deploy/preload-models.py \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*

# Ports freigeben: 8076 (API) und 8077 (Nginx) 
EXPOSE 8076 8077

# Startbefehl: Supervisord übernimmt den Start und die Überwachung beider Prozesse
CMD ["supervisord", "-c", "/etc/supervisor/conf.d/supervisord.conf"]

</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 Maximilian Giesen

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

</file>

<file path="README.md">
# Offline Whisper

A lightweight Docker-based implementation of OpenAI's Whisper model for local speech recognition. This project provides a simple web interface for uploading audio files for transcription.

## Features

- Local deployment of OpenAI's Whisper model
- Web interface for audio file uploads
- Language selection directly in the web interface
- On-demand loading of different Whisper model sizes (tiny to large)
- Docker containerization for easy deployment
- Fully offline capable - no internet connection required after initial setup
- No data persistence - all audio files and transcriptions are deleted after processing

## Usage

### For End Users (Offline Usage)

The pre-built Docker image contains all Whisper models and can be used completely offline.

1. **Pull the Docker image** (requires internet connection once)

   ```bash
   docker pull ghcr.io/mgiesen/offline-whisper:latest
   ```

2. **Run the container** (works offline)

   ```bash
   docker run -d -p 8077:8077 ghcr.io/mgiesen/offline-whisper:latest
   ```

3. **Access the web interface** at `http://localhost:8077`

4. **Offline Deployment**

   You can export the Docker image to use it on offline systems:

   ```bash
   # Save the image to a file
   docker save ghcr.io/mgiesen/offline-whisper:latest > offline-whisper.tar

   # On the offline system
   docker load < offline-whisper.tar
   docker run -d -p 8077:8077 ghcr.io/mgiesen/offline-whisper:latest
   ```

### For Developers

If you want to modify the code before running the application, follow these steps:

1. **Clone the repository**

   ```bash
   git clone https://github.com/mgiesen/offline-whisper.git
   cd offline-whisper
   ```

2. **Build and start the containers** (development mode)

   ```bash
   docker-compose up -d
   ```

   Note: The development build doesn't preload models to save build time. Models will be downloaded on first use.

3. **Build for offline deployment** (includes all models)

   ```bash
   docker-compose -f docker-compose-deploy.yml up -d --build
   ```

4. **Access the web interface** at `http://localhost:8077`

## Web interface

![Image](readme/webapp.png)

## Important Notes

- The web interface lets you select from 5 different model sizes:

  - `tiny`: ~150MB - Fastest, lowest accuracy
  - `base`: ~150MB - Fast with decent accuracy
  - `small`: ~500MB - Good balance of speed and accuracy
  - `medium`: ~1.5GB - Better accuracy, slower
  - `large`: ~6GB - Best accuracy, slowest

- When you switch models, the previously loaded model is unloaded to free up memory
- Models are loaded on-demand (first transcription request) rather than at startup
- The pre-built Docker image includes all model files for offline use
- All uploaded audio files and generated transcripts are automatically deleted after processing for privacy
- No data is stored persistently - everything is processed in memory only

## Technical Details

### Development vs Deployment

- **Development Mode** (`docker-compose.yml`):

  - Faster builds, doesn't download models during build
  - Models are downloaded on first use (requires internet)
  - Ideal for code modifications and testing

- **Deployment Mode** (`docker-compose-deploy.yml`):
  - Includes all Whisper models in the image
  - Can be used completely offline
  - Larger image size (~8GB) but fully portable

## Acknowledgments

This project uses:

- [OpenAI Whisper](https://github.com/openai/whisper)
- [FastAPI](https://fastapi.tiangolo.com/)
- [Nginx](https://nginx.org/)

</file>

<file path=".github/workflows/docker-publish.yml">
name: Publish Docker image

on:
  release:
    types: [published]
  workflow_dispatch:

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to the Container registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push Docker image
        run: |
          docker build -t ghcr.io/mgiesen/offline-whisper:latest .
          docker push ghcr.io/mgiesen/offline-whisper:latest

</file>

<file path="deploy/preload-models.py">
import whisper

models = ["tiny", "base", "small", "medium", "large"]

for model in models:
    print(f"Preloading model {model}...")
    whisper.load_model(model)

</file>

<file path="nginx/Dockerfile">
FROM nginx:alpine

# Remove default nginx configuration
RUN rm /etc/nginx/conf.d/default.conf

# Copy custom nginx configuration
COPY multi-container.conf /etc/nginx/conf.d/default.conf

# Copy static files
COPY public/ /usr/share/nginx/html/

# Expose port
EXPOSE 8077

# Start nginx in foreground
CMD ["nginx", "-g", "daemon off;"]
</file>

<file path="nginx/public/index.html">
<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="UTF-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		<title>Offline Whisper</title>
		<style>
			body {
				position: relative;
				min-height: 100vh;
				padding-top: 80px;
			}
			.title {
				font-size: 2.5rem;
				margin-bottom: 2rem;
				color: #2c3e50;
				font-weight: bold;
			}
			@media (max-width: 768px) {
				.title {
					font-size: 2rem;
					margin-bottom: 1.5rem;
				}
				body {
					padding: 60px 10px 20px;
				}
				.github-link {
					top: 10px;
					right: 10px;
				}
			}
			body {
				font-family: Arial, sans-serif;
				padding: 20px;
				text-align: center;
				max-width: 800px;
				margin: 0 auto;
			}
			.github-link {
				position: absolute;
				top: 20px;
				right: 20px;
				display: flex;
				align-items: center;
				text-decoration: none;
				color: #333;
				transition: opacity 0.3s;
			}
			.github-link:hover {
				opacity: 0.7;
			}
			.github-link svg {
				width: 24px;
				height: 24px;
				margin-right: 8px;
			}
			.controls {
				margin: 20px 0;
			}
			#transcriptionResult {
				margin-top: 20px;
				padding: 15px;
				border: 1px solid #ddd;
				border-radius: 5px;
				display: none;
			}
			.loading {
				display: none;
				margin: 20px auto;
			}
			.loading-spinner {
				border: 4px solid #f3f3f3;
				border-top: 4px solid #3498db;
				border-radius: 50%;
				width: 40px;
				height: 40px;
				animation: spin 1s linear infinite;
				margin: 10px auto;
			}
			@keyframes spin {
				0% {
					transform: rotate(0deg);
				}
				100% {
					transform: rotate(360deg);
				}
			}
			.error-message {
				color: red;
				margin: 10px 0;
				display: none;
			}
			button {
				padding: 10px 20px;
				margin: 5px;
				border: none;
				border-radius: 5px;
				background-color: #3498db;
				color: white;
				cursor: pointer;
			}
			button:disabled {
				background-color: #cccccc;
				cursor: not-allowed;
			}
			select {
				padding: 8px;
				margin: 10px;
				border-radius: 5px;
			}
			.settings {
				margin-bottom: 20px;
				padding: 15px;
				border: 1px solid #eee;
				border-radius: 5px;
				background-color: #f9f9f9;
			}
			.settings label {
				margin-right: 5px;
				font-weight: bold;
			}
			.model-info {
				font-size: 0.8rem;
				margin-top: 5px;
				color: #666;
			}
			.model-loaded {
				color: green;
			}
		</style>
	</head>
	<body>
		<h1 class="title">Offline Whisper</h1>
		<a href="https://github.com/mgiesen/offline-whisper" target="_blank" class="github-link">
			<svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
				<path fill="currentColor" d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z" />
			</svg>
			GitHub
		</a>

		<div class="settings">
			<div>
				<label for="languageSelect">Transcription Language:</label>
				<select id="languageSelect">
					<option value="de">German</option>
					<option value="en">English</option>
					<option value="fr">French</option>
					<option value="es">Spanish</option>
					<option value="it">Italian</option>
					<option value="ja">Japanese</option>
					<option value="zh">Chinese</option>
					<option value="nl">Dutch</option>
					<option value="pt">Portuguese</option>
					<option value="ru">Russian</option>
				</select>
			</div>
			<div style="margin-top: 10px">
				<label for="modelSelect">Whisper Model:</label>
				<select id="modelSelect">
					<option value="tiny">Tiny (~150MB) - Fastest</option>
					<option value="base">Base (~150MB) - Fast</option>
					<option value="small">Small (~500MB) - Balanced</option>
					<option value="medium">Medium (~1.5GB) - Good</option>
					<option value="large">Large (~6GB) - Best Accuracy</option>
				</select>
				<div id="modelInfo" class="model-info"></div>
			</div>
		</div>

		<div class="controls">
			<button id="uploadBtn">Upload Audio File</button>
			<input id="fileInput" type="file" accept="audio/*" style="display: none" />
		</div>

		<div class="loading" id="loadingIndicator">
			<div class="loading-spinner"></div>
			<p>Processing audio...</p>
		</div>

		<div class="error-message" id="errorMessage"></div>

		<div id="transcriptionResult"></div>

		<script>
			const uploadBtn = document.getElementById("uploadBtn");
			const fileInput = document.getElementById("fileInput");
			const loadingIndicator = document.getElementById("loadingIndicator");
			const errorMessage = document.getElementById("errorMessage");
			const transcriptionResult = document.getElementById("transcriptionResult");
			const languageSelect = document.getElementById("languageSelect");
			const modelSelect = document.getElementById("modelSelect");
			const modelInfo = document.getElementById("modelInfo");

			function setControlsState(disabled) {
				uploadBtn.disabled = disabled;
				languageSelect.disabled = disabled;
				modelSelect.disabled = disabled;
			}

			function showError(message) {
				errorMessage.textContent = message;
				errorMessage.style.display = "block";
				setTimeout(() => {
					errorMessage.style.display = "none";
				}, 5000);
			}

			uploadBtn.addEventListener("click", () => fileInput.click());

			fileInput.addEventListener("change", (e) => {
				if (e.target.files.length > 0) {
					processAudioData(e.target.files[0]);
				}
			});

			async function checkCurrentModel() {
				try {
					const response = await fetch("/api/model-info");
					if (!response.ok) {
						throw new Error(`HTTP error! Status: ${response.status}`);
					}

					const modelData = await response.json();
					if (modelData.is_loaded && modelData.current_model) {
						modelSelect.value = modelData.current_model;
						modelInfo.textContent = `Model "${modelData.current_model}" is currently loaded`;
						modelInfo.classList.add("model-loaded");
					} else {
						modelInfo.textContent = "No model currently loaded";
						modelInfo.classList.remove("model-loaded");
					}
				} catch (error) {
					console.error("Error checking model info:", error);
					modelInfo.textContent = "Unable to check model status";
					modelInfo.classList.remove("model-loaded");
				}
			}

			async function processAudioData(audioFile) {
				setControlsState(true);
				loadingIndicator.style.display = "block";
				transcriptionResult.style.display = "none";

				const formData = new FormData();
				formData.append("file", audioFile, audioFile.name);

				const language = languageSelect.value;
				const model = modelSelect.value;

				try {
					const response = await fetch(`/api/transcribe?language=${language}&model_name=${model}`, {
						method: "POST",
						body: formData,
					});

					if (response.status === 423) {
						throw new Error("Service is currently busy. Please try again in a few seconds.");
					}

					if (!response.ok) {
						throw new Error(`HTTP error! Status: ${response.status}`);
					}

					const text = await response.text();
					transcriptionResult.textContent = text;
					transcriptionResult.style.display = "block";

					// Update model info after transcription
					await checkCurrentModel();
				} catch (error) {
					showError("Processing error: " + error.message);
				} finally {
					loadingIndicator.style.display = "none";
					setControlsState(false);
					fileInput.value = "";
				}
			}

			window.addEventListener("DOMContentLoaded", checkCurrentModel);
		</script>
	</body>
</html>

</file>

<file path="whisper-api/Dockerfile">
# Base image with Python 3.9
FROM python:3.9-slim

# System dependencies for Whisper
RUN apt-get update && apt-get install -y ffmpeg

# Set working directory
WORKDIR /app

# Copy and install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Create temporary directory
RUN mkdir -p /tmp

# Copy application code
COPY main.py .

# Start FastAPI application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8076"]

</file>

<file path="whisper-api/main.py">
from fastapi import FastAPI, UploadFile, File, HTTPException, Query
from fastapi.responses import PlainTextResponse
from uuid import uuid4
import os
import whisper
import gc

app = FastAPI()

# Don't load the model at startup
model = None
current_model_name = None

def load_model(model_name: str):
    global model, current_model_name
    
    # If the requested model is already loaded, don't reload it
    if current_model_name == model_name and model is not None:
        return model
    
    # If a different model is loaded, unload it first
    if model is not None:
        print(f"Unloading current model: {current_model_name}")
        del model
        gc.collect()  # Force garbage collection
    
    # Load the requested model
    print(f"Loading Whisper model: {model_name}")
    model = whisper.load_model(model_name)
    current_model_name = model_name
    return model

def transcribe_audio(audio_path: str, model_name: str, language: str = None) -> str:
    try:
        # Load the model (or use already loaded one)
        current_model = load_model(model_name)
        
        # Use provided language or fall back to default English
        lang = language or 'en'
        result = current_model.transcribe(
            audio_path, 
            language=lang
        )
        return result["text"]
    except Exception as e:
        raise Exception(f"Transcription error: {e}")

@app.post("/transcribe", response_class=PlainTextResponse)
async def transcribe(
    file: UploadFile = File(...),
    language: str = Query(None),
    model_name: str = Query("tiny")  # Default to tiny for faster processing
) -> str:
    if not file or not file.filename:
        raise HTTPException(
            status_code=400, 
            detail="No file received or invalid file upload."
        )

    print(f"Received file: {file.filename}, Content-Type: {file.content_type}, Language: {language}, Model: {model_name}")
    
    audio_path = f"/tmp/{uuid4()}_{file.filename}"
    try:
        with open(audio_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        return transcribe_audio(audio_path, model_name, language)
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
    finally:
        if os.path.exists(audio_path):
            os.remove(audio_path)

@app.get("/model-info")
async def model_info() -> dict:
    return {
        "current_model": current_model_name,
        "is_loaded": model is not None,
        "available_models": ["tiny", "base", "small", "medium", "large"]
    }

@app.get("/health")
async def health_check() -> dict:
    return {"status": "healthy"}
</file>

<file path="whisper-api/requirements.txt">
fastapi
uvicorn
openai-whisper
torch
python-multipart
</file>
</file_contents>
</project_overview>